{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Mingang Hao 1326642 mhhao1@student.unimelb.edu.au"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation & Data loading\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HMY\\anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "    # Join tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "path = \"project-data\"\n",
    "train_data = load_data(os.path.join(path,'train-claims.json'))\n",
    "dev_data = load_data(os.path.join(path,'dev-claims.json'))\n",
    "test_data = load_data(os.path.join(path,'test-claims-unlabelled.json'))\n",
    "evidence_data = load_data(os.path.join(path,'evidence.json'))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Prediction on Dev and Test Set\n",
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "for claim_id, claim_data in train_data.items():\n",
    "    train_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "    for i, evidence_id in enumerate(claim_data['evidences']):\n",
    "        train_data[claim_id]['evidences'][i] = preprocess(evidence_data[evidence_id])\n",
    "\n",
    "for claim_id, claim_data in dev_data.items():\n",
    "    dev_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "    for i, evidence_id in enumerate(claim_data['evidences']):\n",
    "        dev_data[claim_id]['evidences'][i] = preprocess(evidence_data[evidence_id])\n",
    "\n",
    "for claim_id, claim_data in test_data.items():\n",
    "    test_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "\n",
    "for ev_id, ev_data in evidence_data.items():\n",
    "    evidence_data[ev_id]= preprocess(ev_data)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def retrieve_evidence(claim_text):\n",
    "    claim_vector = vectorizer.transform([claim_text])\n",
    "    sim_scores = cosine_similarity(tfidf_matrix, claim_vector).flatten()\n",
    "    top_doc_indices = sim_scores.argsort()[::-1][:5] # retrieve top 10 evidence passages\n",
    "    return [doc_ids[i] for i in top_doc_indices]\n",
    "def concat_claim_with_evidence(claim_text, evidence_texts):\n",
    "    return claim_text + ' ' + ' '.join(evidence_texts)\n",
    "\n",
    "corpus = []\n",
    "doc_ids = []\n",
    "for evidence_id, evidence_text in evidence_data.items():\n",
    "    corpus.append(evidence_text)\n",
    "    doc_ids.append(evidence_id)\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "X_train = []\n",
    "for claim_id, claim_data in train_data.items():\n",
    "    claim_text = claim_data['claim_text']\n",
    "    evidence_texts = [evidence_id for evidence_id in claim_data['evidences']]\n",
    "    X_train.append(concat_claim_with_evidence(claim_text, evidence_texts))\n",
    "\n",
    "y_train = [train_data[claim_id]['claim_label'] for claim_id in train_data]\n",
    "\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svc', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions for the dev data\n",
    "dev_predictions = {}\n",
    "final_evidences = []\n",
    "for claim_id, claim_data in dev_data.items():\n",
    "    claim_text = claim_data['claim_text']\n",
    "    indices = retrieve_evidence(claim_text)\n",
    "    evidence_texts = [evidence_data[evidence_id] for evidence_id in indices]\n",
    "    concatenated_text = concat_claim_with_evidence(claim_text, evidence_texts)\n",
    "    y_pred = model.predict([concatenated_text])[0]\n",
    "    for id in indices:\n",
    "        ev_text = evidence_data[id]\n",
    "        concatenated_text = concat_claim_with_evidence(claim_text, ev_text)\n",
    "        if model.predict([concatenated_text])[0] == y_pred:\n",
    "            final_evidences.append(id)\n",
    "    dev_predictions[claim_id] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"claim_label\": y_pred,\n",
    "        \"evidences\": final_evidences\n",
    "    }\n",
    "\n",
    "# Make predictions for the test data\n",
    "test_predictions = {}\n",
    "for claim_id, claim_data in test_data.items():\n",
    "    claim_text = claim_data['claim_text']\n",
    "    indices = retrieve_evidence(claim_text)\n",
    "    evidence_texts = [evidence_data[evidence_id] for evidence_id in indices]\n",
    "    concatenated_text = concat_claim_with_evidence(claim_text, evidence_texts)\n",
    "    y_pred = model.predict([concatenated_text])[0]\n",
    "    \n",
    "    test_predictions[claim_id] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"claim_label\": y_pred,\n",
    "        \"evidences\": indices\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path,'test_predictions_svm.json'), 'w') as f:\n",
    "    json.dump(test_predictions, f)\n",
    "with open(os.path.join(path,'dev_predictions_svm.json'), 'w') as f:\n",
    "    json.dump(dev_predictions, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "path = \"project-data\"\n",
    "train_data = load_data(os.path.join(path,'train-claims.json'))\n",
    "dev_data = load_data(os.path.join(path,'dev-claims.json'))\n",
    "test_data = load_data(os.path.join(path,'test-claims-unlabelled.json'))\n",
    "evidence_data = load_data(os.path.join(path,'evidence.json'))\n",
    "def preprocess(sentence):\n",
    "    sentence = re.sub('[^a-zA-Z0-9]', ' ', sentence)\n",
    "    return  ' '.join([lemmatizer.lemmatize(w) for w in sentence.lower().split() if w not in stop_words])\n",
    "# Preprocess the data\n",
    "for claim_id, claim_data in train_data.items():\n",
    "    train_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "    for i, evidence_id in enumerate(claim_data['evidences']):\n",
    "        train_data[claim_id]['evidences'][i] = preprocess(evidence_data[evidence_id])\n",
    "\n",
    "for claim_id, claim_data in dev_data.items():\n",
    "    dev_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "    for i, evidence_id in enumerate(claim_data['evidences']):\n",
    "        dev_data[claim_id]['evidences'][i] = preprocess(evidence_data[evidence_id])\n",
    "\n",
    "for claim_id, claim_data in test_data.items():\n",
    "    test_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "\n",
    "\n",
    "import gensim.downloader as api\n",
    "wmd_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = []\n",
    "doc_ids = []\n",
    "for evidence_id, evidence_text in evidence_data.items():\n",
    "    corpus.append(preprocess(evidence_text))\n",
    "    doc_ids.append(evidence_id)\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_evidence(claim_text):\n",
    "    claim_vector = vectorizer.transform([claim_text])\n",
    "    sim_scores = cosine_similarity(tfidf_matrix, claim_vector).flatten()    \n",
    "    top_doc_indices = sim_scores.argsort()[::-1][:100] # retrieve top 100 evidence passages\n",
    "    ev_text = [corpus[i] for i in top_doc_indices]    \n",
    "    wmd_distances = []\n",
    "    for ev in ev_text:\n",
    "        wmd_distances.append(wmd_model.wmdistance(claim_text.split(),ev.split()))\n",
    "    wmd_indices_sorted = np.array(wmd_distances).argsort()[:3]  # Smaller distances are better\n",
    "    top_wmd_indices = [top_doc_indices[i] for i in wmd_indices_sorted]\n",
    "    #print(top_wmd_indices)\n",
    "    return [doc_ids[i] for i in top_wmd_indices]\n",
    "\n",
    "\n",
    "def concat_claim_with_evidence(claim_text, evidence_texts):\n",
    "    return preprocess(claim_text + ' ' + ' '.join(evidence_texts))\n",
    "\n",
    "\n",
    "X_train = []\n",
    "for claim_id, claim_data in train_data.items():\n",
    "    claim_text = claim_data['claim_text']\n",
    "    evidence_texts = [evidence_id for evidence_id in claim_data['evidences']]\n",
    "    X_train.append(concat_claim_with_evidence(claim_text, evidence_texts))\n",
    "\n",
    "y_train = [train_data[claim_id]['claim_label'] for claim_id in train_data]\n",
    "\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svc', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions for the dev data\n",
    "for claim_id, claim_data in dev_data.items():\n",
    "    claim_text = claim_data['claim_text']\n",
    "    indices = retrieve_evidence(claim_text)\n",
    "    evidence_texts = [evidence_data[evidence_id] for evidence_id in indices]\n",
    "    concatenated_text = concat_claim_with_evidence(claim_text, evidence_texts)\n",
    "    y_pred = model.predict([concatenated_text])[0]\n",
    "    dev_predictions[claim_id] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"claim_label\": y_pred,\n",
    "        \"evidences\": indices\n",
    "    }\n",
    "\n",
    "# Make predictions for the test data\n",
    "test_predictions = {}\n",
    "for claim_id, claim_data in test_data.items():\n",
    "    claim_text = claim_data['claim_text']\n",
    "    indices = retrieve_evidence(claim_text)\n",
    "    evidence_texts = [evidence_data[evidence_id] for evidence_id in indices]\n",
    "    concatenated_text = concat_claim_with_evidence(claim_text, evidence_texts)\n",
    "    y_pred = model.predict([concatenated_text])[0]\n",
    "    \n",
    "    test_predictions[claim_id] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"claim_label\": y_pred,\n",
    "        \"evidences\": indices\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path,'test_predictions_wmd.json'), 'w') as f:\n",
    "    json.dump(test_predictions, f)\n",
    "with open(os.path.join(path,'dev_predictions_wmd.json'), 'w') as f:\n",
    "    json.dump(dev_predictions, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "39/39 [==============================] - 3s 45ms/step - loss: 1.3048 - accuracy: 0.4178 - val_loss: 1.2801 - val_accuracy: 0.4416\n",
      "Epoch 2/10\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 1.2115 - accuracy: 0.4235 - val_loss: 1.2774 - val_accuracy: 0.4221\n",
      "Epoch 3/10\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 1.0284 - accuracy: 0.5790 - val_loss: 1.4814 - val_accuracy: 0.2857\n",
      "Epoch 4/10\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.7531 - accuracy: 0.7191 - val_loss: 2.0275 - val_accuracy: 0.1753\n",
      "Epoch 5/10\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.5619 - accuracy: 0.7875 - val_loss: 1.5063 - val_accuracy: 0.3052\n",
      "Epoch 6/10\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.4242 - accuracy: 0.8607 - val_loss: 2.3433 - val_accuracy: 0.1948\n",
      "Epoch 7/10\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.3456 - accuracy: 0.8656 - val_loss: 2.1776 - val_accuracy: 0.2143\n",
      "Epoch 8/10\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2843 - accuracy: 0.9121 - val_loss: 2.6431 - val_accuracy: 0.1948\n",
      "Epoch 9/10\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2079 - accuracy: 0.9308 - val_loss: 2.1938 - val_accuracy: 0.2338\n",
      "Epoch 10/10\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1893 - accuracy: 0.9438 - val_loss: 1.9285 - val_accuracy: 0.2662\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "path = \"project-data\"\n",
    "train_data = json.load(open(os.path.join(path, \"train-claims.json\")))\n",
    "dev_data = json.load(open(os.path.join(path, \"dev-claims.json\")))\n",
    "test_data = json.load(open(os.path.join(path, \"test-claims-unlabelled.json\")))\n",
    "evidence_data = json.load(open(os.path.join(path, \"evidence.json\")))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "    # Join tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "def retrieve_evidence(claim_text):\n",
    "    claim_vector = vectorizer.transform([claim_text])\n",
    "    sim_scores = cosine_similarity(tfidf_matrix, claim_vector).flatten()\n",
    "    top_doc_indices = sim_scores.argsort()[::-1][:5] # retrieve top 10 evidence passages\n",
    "    return [doc_ids[i] for i in top_doc_indices]\n",
    "\n",
    "train_claims = []\n",
    "train_labels = []\n",
    "\n",
    "for claim_id, claim_data in train_data.items():\n",
    "    claim_text = preprocess(claim_data['claim_text'])\n",
    "    evidence_ids = retrieve_evidence(claim_text)\n",
    "    evidence_texts = [preprocess(evidence_data[evidence_id]) for evidence_id in evidence_ids]\n",
    "    concatenated_text = concat_claim_with_evidence(claim_text, evidence_texts)\n",
    "\n",
    "    train_claims.append(concatenated_text)\n",
    "    train_labels.append(claim_data['claim_label'])\n",
    "\n",
    "\n",
    "dev_claims = []\n",
    "dev_labels = []\n",
    "for claim_id, claim_data in dev_data.items():\n",
    "    dev_claims.append(preprocess(claim_data['claim_text']))    \n",
    "    dev_labels.append(claim_data['claim_label'])\n",
    "\n",
    "test_claims = []\n",
    "for claim_id, claim_data in test_data.items():\n",
    "    test_claims.append(preprocess(claim_data['claim_text']))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_claims)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_claims)\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_claims)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_claims)\n",
    "\n",
    "# Find the maximum sequence length\n",
    "max_length = max(max([len(seq) for seq in train_sequences]),\n",
    "                 max([len(seq) for seq in dev_sequences]),\n",
    "                 max([len(seq) for seq in test_sequences]))\n",
    "\n",
    "# Pad your sequences so that they all have the same length\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_length)\n",
    "dev_sequences = pad_sequences(dev_sequences, maxlen=max_length)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels)\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "dev_labels_encoded = label_encoder.transform(dev_labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Define the model\n",
    "max_length = max([len(seq) for seq in np.concatenate((train_sequences, dev_sequences, test_sequences))])\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64, input_length=max_length))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_sequences, train_labels_encoded, epochs=10, validation_data=(dev_sequences, dev_labels_encoded))\n",
    "\n",
    "# Make predictions\n",
    "# Make predictions\n",
    "dev_predictions = {}\n",
    "for claim_id, claim_data in dev_data.items():\n",
    "    claim_text = preprocess(claim_data['claim_text'])\n",
    "    evidence_ids = retrieve_evidence(claim_text)\n",
    "    evidence_texts = [preprocess(evidence_data[evidence_id]) for evidence_id in evidence_ids]\n",
    "    concatenated_text = concat_claim_with_evidence(claim_text, evidence_texts)\n",
    "\n",
    "    # Prepare the concatenated text for prediction\n",
    "    sequence = tokenizer.texts_to_sequences([concatenated_text])\n",
    "    sequence = pad_sequences(sequence, maxlen=max_length)\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(sequence)\n",
    "    y_pred_label_encoded = np.argmax(prediction, axis=1)[0]\n",
    "    y_pred_label = label_encoder.inverse_transform([y_pred_label_encoded])[0]\n",
    "    \n",
    "    dev_predictions[claim_id] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"claim_label\": y_pred_label,\n",
    "        \"evidences\": evidence_ids\n",
    "    }\n",
    "test_predictions = {}\n",
    "for claim_id, claim_data in test_data.items():\n",
    "    claim_text = preprocess(claim_data['claim_text'])\n",
    "    evidence_ids = retrieve_evidence(claim_text)\n",
    "    evidence_texts = [preprocess(evidence_data[evidence_id]) for evidence_id in evidence_ids]\n",
    "    concatenated_text = concat_claim_with_evidence(claim_text, evidence_texts)\n",
    "\n",
    "    # Prepare the concatenated text for prediction\n",
    "    sequence = tokenizer.texts_to_sequences([concatenated_text])\n",
    "    sequence = pad_sequences(sequence, maxlen=max_length)\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(sequence)\n",
    "    y_pred_label_encoded = np.argmax(prediction, axis=1)[0]\n",
    "    y_pred_label = label_encoder.inverse_transform([y_pred_label_encoded])[0]\n",
    "    \n",
    "    test_predictions[claim_id] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"claim_label\": y_pred_label,\n",
    "        \"evidences\": evidence_ids\n",
    "    }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path,'test_predictions_lstm.json'), 'w') as f:\n",
    "    json.dump(test_predictions, f)\n",
    "with open(os.path.join(path,'dev_predictions_lstm.json'), 'w') as f:\n",
    "    json.dump(dev_predictions, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec+cosine sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "word_vectors = api.load(\"word2vec-google-news-300\") # Load pre-trained word vectors\n",
    "\n",
    "def get_word_embeddings(text):\n",
    "    embeddings = []\n",
    "    for word in text.split():\n",
    "        if word in word_vectors:\n",
    "            embeddings.append(word_vectors[word])\n",
    "    if len(embeddings) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "\n",
    "def retrieve_evidence(claim_text, evidence_data):\n",
    "    sim_scores = []\n",
    "    for evidence_id, evidence_text in evidence_data.items():\n",
    "        claim_emb = get_word_embeddings(claim_text)\n",
    "        evidence_emb = get_word_embeddings(evidence_text)\n",
    "        if claim_emb is None or evidence_emb is None:\n",
    "            sim_scores.append(0)\n",
    "        else:\n",
    "            sim_scores.append(cosine_similarity([claim_emb], [evidence_emb])[0][0])\n",
    "    top_doc_indices = np.argsort(sim_scores)[::-1][:5] # retrieve top 5 evidence passages\n",
    "    return [list(evidence_data.keys())[i] for i in top_doc_indices]\n",
    "\n",
    "X_train = [train_data[claim_id]['claim_text'] for claim_id in train_data]\n",
    "y_train = [train_data[claim_id]['claim_label'] for claim_id in train_data]\n",
    "\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svc', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "dev_predictions = {}\n",
    "for claim_id, claim_data in dev_data.items():\n",
    "    claim_text = claim_data['claim_text']\n",
    "    evidences = retrieve_evidence(claim_text, evidence_data)\n",
    "    evidence_texts = [evidence_data[eid] for eid in evidences]\n",
    "    evidence_text = ' '.join(evidence_texts)\n",
    "    y_pred = model.predict([claim_text])[0]\n",
    "    \n",
    "    dev_predictions[claim_id] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"claim_label\": y_pred,\n",
    "        \"evidences\": evidences\n",
    "    }\n",
    "test_predictions = {}\n",
    "for claim_id, claim_data in test_data.items():\n",
    "    claim_text = claim_data['claim_text']\n",
    "    evidences = retrieve_evidence(claim_text, evidence_data)\n",
    "    evidence_texts = [evidence_data[eid] for eid in evidences]\n",
    "    evidence_text = ' '.join(evidence_texts)\n",
    "    y_pred = model.predict([claim_text])[0]\n",
    "    \n",
    "    test_predictions[claim_id] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"claim_label\": y_pred,\n",
    "        \"evidences\": evidences\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path,'test_predictions_w2v.json'), 'w') as f:\n",
    "    json.dump(test_predictions, f)\n",
    "with open(os.path.join(path,'dev_predictions_w2v.json'), 'w') as f:\n",
    "    json.dump(dev_predictions, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code block onwards are illustration of my experiment, not all of them will generate test_prediction in correct format. \n",
    "--------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of IDs and a list of evidence data\n",
    "ids = list(evidence_data.keys())\n",
    "evidence_list = list(evidence_data.values())\n",
    "\n",
    "# create a pandas dataframe with two columns\n",
    "evidence_df = pd.DataFrame({'id': ids, 'evidence': evidence_list})\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(train_data).transpose()\n",
    "dev_df = pd.DataFrame.from_dict(dev_data).transpose()\n",
    "test_df = pd.DataFrame.from_dict(test_data).transpose()\n",
    "# Define the label-to-integer mapping\n",
    "label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n",
    "\n",
    "# Convert the labels to integers\n",
    "train_df['claim_label'] = train_df['claim_label'].replace(label_map)\n",
    "dev_df['claim_label'] = dev_df['claim_label'].replace(label_map)\n",
    "# create a list of IDs and a list of evidence data\n",
    "ids = list(evidence_data.keys())\n",
    "evidence_list = list(evidence_data.values())\n",
    "\n",
    "# create a pandas dataframe with two columns\n",
    "evidence_df = pd.DataFrame({'id': ids, 'evidence': evidence_list})\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(train_data).transpose()\n",
    "dev_df = pd.DataFrame.from_dict(dev_data).transpose()\n",
    "test_df = pd.DataFrame.from_dict(test_data).transpose()\n",
    "# Define the label-to-integer mapping\n",
    "label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n",
    "\n",
    "# Convert the labels to integers\n",
    "train_df['claim_label'] = train_df['claim_label'].replace(label_map)\n",
    "dev_df['claim_label'] = dev_df['claim_label'].replace(label_map)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune the BERT model on the training set using a fact-checking objective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [{\"id\": k, **v} for k, v in train_data.items()]\n",
    "train_sp, temp_data = train_test_split(data_list, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "bert_name = \"bert-base-uncased\"\n",
    "num_labels = 4  # Number of labels (e.g., SUPPORTS, REFUTES, NOT RELATED)\n",
    "\n",
    "class FactCheckingDataset(Dataset):\n",
    "    def __init__(self, claims_data, evidence_data, tokenizer):\n",
    "        self.claims_data = claims_data\n",
    "        self.evidence_data = evidence_data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claims_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        claim = self.claims_data[idx]\n",
    "        claim_text = claim[\"claim_text\"]\n",
    "        evidence_texts = [self.evidence_data[evidence_id] for evidence_id in claim[\"evidences\"]]\n",
    "        concatenated_text = claim_text + \" \" + \" \".join(evidence_texts)\n",
    "        label = label_map[claim[\"claim_label\"]]\n",
    "        encoding = self.tokenizer(concatenated_text, truncation=True, padding=\"max_length\", max_length=512, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label),\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize the BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(bert_name, num_labels=4) \n",
    "tokenizer = BertTokenizer.from_pretrained(bert_name)\n",
    "label_map = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT_ENOUGH_INFO\": 2, \"DISPUTED\": 3}\n",
    "\n",
    "# Convert the training and validation data to Dataset format\n",
    "train_dataset = FactCheckingDataset(train_sp, evidence_data, tokenizer)\n",
    "val_dataset = FactCheckingDataset(val_data, evidence_data, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# Set the model to use the GPU, if available\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    seed=42,\n",
    "    learning_rate=1e-5,\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "output_dir = \"finetuned_bert\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuned Bert, Top 10 Cosine, SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at finetuned_bert were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load fine-tuned BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained('finetuned_bert')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('finetuned_bert')\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(texts, batch_size=4):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = bert_tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        outputs = bert_model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].detach().numpy()  # We take the embeddings from the [CLS] token\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.concatenate(embeddings)\n",
    "\n",
    "def retrieve_evidence(claim_text):\n",
    "    claim_embedding = get_bert_embeddings([claim_text])\n",
    "    sim_scores = cosine_similarity(evidence_embeddings, claim_embedding)\n",
    "    top_doc_indices = sim_scores.flatten().argsort()[::-1][:5]  # retrieve top 5 evidence passages\n",
    "    return [list(evidence_data.keys())[i] for i in top_doc_indices]\n",
    "\n",
    "\n",
    "\n",
    "evidence_texts = list(evidence_data.values())\n",
    "evidence_embeddings = get_bert_embeddings(evidence_texts)\n",
    "\n",
    "\n",
    "X_train = [train_data[claim_id]['claim_text'] for claim_id in train_data]\n",
    "y_train = [train_data[claim_id]['claim_label'] for claim_id in train_data]\n",
    "\n",
    "# Get BERT embeddings for the train data\n",
    "X_train = get_bert_embeddings(X_train)\n",
    "\n",
    "# Train the model with BERT embeddings\n",
    "model = make_pipeline(StandardScaler(), SVC(kernel='rbf'))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for the dev data\n",
    "dev_predictions = {}\n",
    "for claim_id, claim_data in dev_data.items():\n",
    "    claim_text = claim_data['claim_text']\n",
    "    evidences = retrieve_evidence(claim_text)\n",
    "    claim_embedding = get_bert_embeddings([claim_text])\n",
    "    y_pred = model.predict(claim_embedding)[0]\n",
    "    \n",
    "    dev_predictions[claim_id] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"claim_label\": y_pred,\n",
    "        \"evidences\": evidences\n",
    "    }\n",
    "\n",
    "# Make predictions for the test data\n",
    "test_predictions = {}\n",
    "for claim_id, claim_data in test_data.items():\n",
    "    claim_text = claim_data['claim_text']\n",
    "    evidences = retrieve_evidence(claim_text)\n",
    "    claim_embedding = get_bert_embeddings([claim_text])\n",
    "    y_pred = model.predict(claim_embedding)[0]\n",
    "    \n",
    "    test_predictions[claim_id] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"claim_label\": y_pred,\n",
    "        \"evidences\": evidences\n",
    "    }  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_dataloader, optimizer, scheduler, device)\n",
    "    dev_loss = evaluate(model, dev_dataloader, device)\n",
    "    print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Dev Loss: {dev_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evidence_embeddings(corpus, batch_size=32):\n",
    "    evidence_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(corpus), batch_size):\n",
    "        batch = corpus[i:i + batch_size]\n",
    "        batch_embeddings = [get_claim_embedding(evidence_text) for evidence_text in batch]\n",
    "        evidence_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return evidence_embeddings\n",
    "\n",
    "evidence_embeddings = generate_evidence_embeddings(corpus)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set inputs for Grid Search\n",
    "**Please replace the  fine_tuned_model_path with the fine-tuned transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Input,concatenate\n",
    "from sentence_transformers import SentenceTransformer,models\n",
    "# Create the input layers\n",
    "claim_input = Input(shape=(768,))\n",
    "evidence_input = Input(shape=(768,))\n",
    "\n",
    "#change this name to the fine-tuned transformer\n",
    "fine_tuned_model_path = \"finetuned_bert\"\n",
    "\n",
    "# Load your fine-tuned model as a Sentence Transformer model\n",
    "word_embedding_model = models.Transformer(fine_tuned_model_path)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=True)\n",
    "\n",
    "# Create the Sentence Transformer model\n",
    "device = 'cuda'\n",
    "sentence_transformer = SentenceTransformer(modules=[word_embedding_model, pooling_model], device=device)\n",
    "\n",
    "# Now you can use the model to encode sentences to vectors\n",
    "evidence_embeddings = sentence_transformer.encode(evidence_data)\n",
    "# Concatenate the claim and evidence input layers\n",
    "combined_input = concatenate([claim_input, evidence_input])\n",
    "claim_embeddings_train = sentence_transformer.encode(train_df['claim_text'].tolist())\n",
    "evidence_embeddings_train = []\n",
    "\n",
    "for e in train_df['evidences']:\n",
    "    evidence_contents = evidence_df[evidence_df['id'].isin(e)]['evidence'].tolist()\n",
    "    evidence_content_embeddings = sentence_transformer.encode(evidence_contents)\n",
    "    evidence_embeddings_train.append(evidence_content_embeddings)\n",
    "\n",
    "# Make sure both lists are NumPy arrays\n",
    "claim_embeddings_train = np.array(claim_embeddings_train)\n",
    "\n",
    "# Concatenate claim and evidence embeddings in training data\n",
    "evidence_embeddings_train_mean = [np.mean(evidence_emb, axis=0) for evidence_emb in evidence_embeddings_train]\n",
    "X_train = np.array([np.hstack((claim_emb, evidence_emb)) for claim_emb, evidence_emb in zip(claim_embeddings_train, evidence_embeddings_train_mean)])\n",
    "\n",
    "# Preprocess the concatenated embeddings using StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "y_train = train_df['claim_label']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Prepare dev set embeddings\n",
    "claim_embeddings_dev = sentence_transformer.encode(dev_df['claim_text'].tolist())\n",
    "evidence_embeddings_dev = []\n",
    "\n",
    "for e in dev_df['evidences']:\n",
    "    evidence_contents = evidence_df[evidence_df['id'].isin(e)]['evidence'].tolist()\n",
    "    evidence_content_embeddings = sentence_transformer.encode(evidence_contents)\n",
    "    evidence_embeddings_dev.append(evidence_content_embeddings)\n",
    "\n",
    "# Concatenate claim and evidence embeddings in your dev data\n",
    "evidence_embeddings_dev_mean = [np.mean(evidence_emb, axis=0) for evidence_emb in evidence_embeddings_dev]\n",
    "X_dev = np.array([np.hstack((claim_emb, evidence_emb)) for claim_emb, evidence_emb in zip(claim_embeddings_dev, evidence_embeddings_dev_mean)])\n",
    "\n",
    "# Preprocess the concatenated embeddings using StandardScaler\n",
    "X_dev = scaler.transform(X_dev)\n",
    "# Calculate the confusion matrix\n",
    "y_dev_true = dev_df['claim_label']\n",
    "# Tune hyperparameters using grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'max_iter': [500, 1000, 2000, 4000],\n",
    "}\n",
    "\n",
    "logreg_cv = LogisticRegression(multi_class=\"ovr\", solver='lbfgs')\n",
    "grid_search = GridSearchCV(logreg_cv, param_grid, scoring='accuracy', cv=5)\n",
    "try:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    # Find the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "    # Train the model using the best hyperparameters\n",
    "    best_logreg = LogisticRegression(**best_params, multi_class=\"ovr\", solver='lbfgs')    \n",
    "except Exception as e:\n",
    "    print(\"Error during grid search:\", e)\n",
    "    best_logreg = logreg_cv\n",
    "    # Predict the labels for the dev set using the best model\n",
    "best_logreg.fit(X_train, y_train)\n",
    "y_dev_pred_best = best_logreg.predict(X_dev)\n",
    "\n",
    "# Calculate the accuracy for the best model\n",
    "accuracy_best = accuracy_score(y_dev_true, y_dev_pred_best)\n",
    "print(\"Accuracy on the dev set using the best model: {:.4f}\".format(accuracy_best))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM model\n",
    "svm = SVC(kernel='linear', decision_function_shape='ovr')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the dev set\n",
    "y_dev_pred_svm = svm.predict(X_dev)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_svm = accuracy_score(y_dev_true, y_dev_pred_svm)\n",
    "print(\"Accuracy on the dev set (SVM): {:.2f}\".format(accuracy_svm))\n",
    "\n",
    "# Tune hyperparameters using grid search or other optimization techniques\n",
    "param_grid_svm = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'decision_function_shape': ['ovr']\n",
    "}\n",
    "\n",
    "svm_cv = SVC()\n",
    "grid_search_svm = GridSearchCV(svm_cv, param_grid_svm, scoring='accuracy', cv=5)\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Find the best hyperparameters\n",
    "best_params_svm = grid_search_svm.best_params_\n",
    "print(\"Best hyperparameters (SVM):\", best_params_svm)\n",
    "\n",
    "# Train the model using the best hyperparameters\n",
    "best_svm = SVC(**best_params_svm)\n",
    "best_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the dev set using the best model\n",
    "y_dev_pred_best_svm = best_svm.predict(X_dev)\n",
    "\n",
    "# Calculate the accuracy for the best model\n",
    "accuracy_best_svm = accuracy_score(y_dev_true, y_dev_pred_best_svm)\n",
    "print(\"Accuracy on the dev set using the best model (SVM): {:.2f}\".format(accuracy_best_svm))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Train a random forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the dev set\n",
    "y_dev_pred_rf = rf.predict(X_dev)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_rf = accuracy_score(y_dev_true, y_dev_pred_rf)\n",
    "print(\"Accuracy on the dev set (Random Forest): {:.2f}\".format(accuracy_rf))\n",
    "\n",
    "# Tune hyperparameters using grid search\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_cv = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(rf_cv, param_grid_rf, scoring='accuracy', cv=5)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Find the best hyperparameters\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "print(\"Best hyperparameters (Random Forest):\", best_params_rf)\n",
    "\n",
    "# Train the model using the best hyperparameters\n",
    "best_rf = RandomForestClassifier(**best_params_rf, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the dev set using the best model\n",
    "y_dev_pred_best_rf = best_rf.predict(X_dev)\n",
    "\n",
    "# Calculate the accuracy for the best model\n",
    "accuracy_best_rf = accuracy_score(y_dev_true, y_dev_pred_best_rf)\n",
    "print(\"Accuracy on the dev set using the best model (Random Forest): {:.2f}\".format(accuracy_best_rf))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available.\")\n",
    "    device = \"/GPU:0\"\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "    device = \"/CPU:0\"\n",
    "def create_rnn_model(lstm_units=50, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, input_shape=(2, 768), activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "X_train_rnn = X_train.reshape(X_train.shape[0], 2, 768)\n",
    "y_train_onehot = pd.get_dummies(train_df['claim_label']).to_numpy()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rnn_model = KerasClassifier(build_fn=create_rnn_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "param_grid_lstm = {\n",
    "    'lstm_units': [50, 100, 150],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'epochs': [10, 20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rnn_model, param_grid=param_grid_lstm, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_rnn, y_train_onehot)\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters (RNN):\", best_params)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Preprocess the dev set\n",
    "X_dev_rnn = X_dev.reshape(X_dev.shape[0], 2, 768)\n",
    "y_dev_true = dev_df['claim_label']\n",
    "\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_rnn_model = create_rnn_model(lstm_units=best_params['lstm_units'], dropout_rate=best_params['dropout_rate'])\n",
    "with tf.device(device):\n",
    "    best_rnn_model.fit(X_train_rnn, y_train_onehot, epochs=best_params['epochs'], batch_size=32, verbose=0)\n",
    "\n",
    "# Predict the labels for the dev set\n",
    "y_dev_pred_proba = best_rnn_model.predict(X_dev_rnn)\n",
    "y_dev_pred_rnn = np.argmax(y_dev_pred_proba, axis=1)\n",
    "\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_rnn = accuracy_score(y_dev_true, y_dev_pred_rnn)\n",
    "print(\"Accuracy on the dev set (RNN): {:.2f}\".format(accuracy_rnn))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Dropout\n",
    "\n",
    "# Define the RNN model\n",
    "def create_rnn_model(lstm_units=50, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(lstm_units, input_shape=(2, 768), activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "rnn_model = KerasClassifier(build_fn=create_rnn_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Define hyperparameters to search over\n",
    "param_grid_rnn = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [10, 20, 30],\n",
    "    'dropout_rate': [0.2, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "# Use grid search to find the best hyperparameters\n",
    "grid_search_rnn = GridSearchCV(estimator=model, param_grid=param_grid_rnn, scoring='accuracy', cv=5)\n",
    "grid_search_rnn.fit(X_train, y_train)\n",
    "\n",
    "# Find the best hyperparameters\n",
    "best_params_rnn = grid_search_rnn.best_params_\n",
    "print(\"Best hyperparameters (RNN):\", best_params_rnn)\n",
    "\n",
    "# Train the RNN model using the best hyperparameters\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(best_params_rnn['dropout_rate']))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=best_params_rnn['batch_size'], epochs=best_params_rnn['epochs'])\n",
    "\n",
    "# Predict the labels for the dev set using the best model\n",
    "y_dev_pred_rnn = model.predict_classes(X_dev)\n",
    "\n",
    "# Calculate the accuracy for the best model\n",
    "accuracy_best_rnn = accuracy_score(y_dev_true, y_dev_pred_rnn)\n",
    "print(\"Accuracy on the dev set using the best model (RNN): {:.2f}\".format(accuracy_best_rnn))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "path = \"project-data\"\n",
    "train_data = load_data(os.path.join(path,'train-claims.json'))\n",
    "dev_data = load_data(os.path.join(path,'dev-claims.json'))\n",
    "test_data = load_data(os.path.join(path,'test-claims-unlabelled.json'))\n",
    "evidence_data = load_data(os.path.join(path,'evidence.json'))\n",
    "def preprocess(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "    # Join tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "for claim_id, claim_data in train_data.items():\n",
    "    train_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "    for i, evidence_id in enumerate(claim_data['evidences']):\n",
    "        train_data[claim_id]['evidences'][i] = preprocess(evidence_data[evidence_id])\n",
    "\n",
    "for claim_id, claim_data in dev_data.items():\n",
    "    dev_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "    for i, evidence_id in enumerate(claim_data['evidences']):\n",
    "        dev_data[claim_id]['evidences'][i] = preprocess(evidence_data[evidence_id])\n",
    "\n",
    "for claim_id, claim_data in test_data.items():\n",
    "    test_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "roberta.to(device)\n",
    "# Create the corpus and doc_ids list from evidence_data ([100000,200000] records)\n",
    "corpus = []\n",
    "doc_ids = []\n",
    "for i, (evidence_id, evidence_text) in enumerate(evidence_data.items()):\n",
    "    corpus.append(preprocess(evidence_text))\n",
    "    doc_ids.append(evidence_id)\n",
    "    \n",
    "def get_claim_embedding(claim_text):\n",
    "    inputs = tokenizer(claim_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = roberta(**inputs)\n",
    "    return outputs[1].cpu().numpy()\n",
    "\n",
    "def generate_evidence_embeddings(corpus, batch_size=32):\n",
    "    evidence_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(corpus), batch_size):\n",
    "        batch = corpus[i:i + batch_size]\n",
    "        batch_embeddings = [get_claim_embedding(evidence_text) for evidence_text in batch]\n",
    "        evidence_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return evidence_embeddings\n",
    "# Generate evidence embeddings\n",
    "evidence_embeddings = generate_evidence_embeddings(corpus)\n",
    "\n",
    "np.save('corpus',corpus)\n",
    "np.save('doc_ids',doc_ids)\n",
    "np.save('evidence_embeddings_all',evidence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_embeddings_all = np.load('evidence_embeddings_all.npy')\n",
    "evidence_embeddings_all= np.vstack(evidence_embeddings_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "k = 20\n",
    "train_data = load_data(os.path.join(path,'train-claims.json'))\n",
    "dev_data = load_data(os.path.join(path,'dev-claims.json'))\n",
    "test_data = load_data(os.path.join(path,'test-claims-unlabelled.json'))\n",
    "evidence_data = load_data(os.path.join(path,'evidence.json'))\n",
    "\n",
    "for claim_id, claim_data in train_data.items():\n",
    "    train_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "    for i, evidence_id in enumerate(claim_data['evidences']):\n",
    "        train_data[claim_id]['evidences'][i] = re.sub(\"[^0-9]\", \"\", evidence_id)\n",
    "for claim_id, claim_data in dev_data.items():\n",
    "    dev_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "    for i, evidence_id in enumerate(claim_data['evidences']):\n",
    "        dev_data[claim_id]['evidences'][i] = re.sub(\"[^0-9]\", \"\", evidence_id)\n",
    "\n",
    "for claim_id, claim_data in test_data.items():\n",
    "    test_data[claim_id]['claim_text'] = re.sub(\"[^0-9]\", \"\", evidence_id)\n",
    "\n",
    "def train_svm_classifier(train_data, evidence_embeddings_all, k=15):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for claim_id in train_data:\n",
    "        claim_emb = get_claim_embedding(preprocess(train_data[claim_id]['claim_text'])).reshape(-1)\n",
    "        claim_label = train_data[claim_id]['claim_label']\n",
    "\n",
    "        # Calculate cosine similarity between the claim and all evidence embeddings\n",
    "        cosine_sims = cosine_similarity([claim_emb], evidence_embeddings_all)[0]\n",
    "        \n",
    "        # Select the top-k evidence embeddings\n",
    "        top_k_indices = np.argsort(cosine_sims)[-k:]\n",
    "        top_k_evidence_embeddings = evidence_embeddings_all[top_k_indices]\n",
    "\n",
    "        # Concatenate the claim embedding with the top-k evidence embeddings\n",
    "        concatenated_embeddings = np.hstack((np.tile(claim_emb, (k, 1)), top_k_evidence_embeddings))\n",
    "\n",
    "        X_train.extend(concatenated_embeddings)\n",
    "        y_train.extend([claim_label] * k)\n",
    "\n",
    "    svm_clf = SVC(kernel='rbf')\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    return svm_clf\n",
    "\n",
    "\n",
    "\n",
    "svm_clf = train_svm_classifier(train_data, evidence_embeddings_all)\n",
    "dump(svm_clf, 'svm_model.joblib')\n",
    "\n",
    "print(dev_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained SVC and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = load('svm_model.joblib')\n",
    "label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n",
    "inv_label_map = {v: key for key, v in label_map.items()}\n",
    "k = 20\n",
    "def predict_labels(dev_data, svm_clf, evidence_embeddings_all):\n",
    "    X_dev = [get_claim_embedding(dev_data[claim_id]['claim_text']).reshape(-1) for claim_id in dev_data]\n",
    "    selected_evidence_indices_dev = []\n",
    "    for claim_emb in X_dev:\n",
    "        cosine_sims = cosine_similarity([claim_emb], evidence_embeddings_all)[0]\n",
    "        top_k_indices = np.argsort(cosine_sims)[-15:]\n",
    "        selected_evidence_indices_dev.append(top_k_indices)\n",
    "    dev_predictions = {}\n",
    "\n",
    "    for i, claim_id in enumerate(dev_data):\n",
    "        claim_data = dev_data[claim_id]\n",
    "        claim_text = claim_data['claim_text']\n",
    "\n",
    "        top_k_evidence_indices = selected_evidence_indices_dev[i]\n",
    "        top_k_evidence_embeddings = evidence_embeddings_all[top_k_evidence_indices]\n",
    "\n",
    "        concatenated_embeddings = [np.hstack((X_dev[i], evidence_emb)) for evidence_emb in top_k_evidence_embeddings]\n",
    "\n",
    "        y_pred_top_k = svm_clf.predict(concatenated_embeddings)\n",
    "        top_k_similarity_scores = cosine_similarity([X_dev[i]], top_k_evidence_embeddings)[0]\n",
    "\n",
    "        weighted_vote = {}\n",
    "        for label, similarity_score in zip(y_pred_top_k, top_k_similarity_scores):\n",
    "            if label in weighted_vote:\n",
    "                weighted_vote[label] += similarity_score \n",
    "            else:\n",
    "                weighted_vote[label] = similarity_score \n",
    "        \n",
    "        y_pred_final = max(weighted_vote, key=weighted_vote.get)\n",
    "\n",
    "        filtered_evidences = [top_k_evidence_indices[j] for j, label in enumerate(y_pred_top_k) if label == y_pred_final]\n",
    "\n",
    "        dev_predictions[claim_id] = {\n",
    "            \"claim_text\": claim_text,\n",
    "            \"claim_label\": y_pred_final,\n",
    "            \"evidences\": filtered_evidences\n",
    "        }\n",
    "\n",
    "    return dev_predictions    \n",
    "dev_predictions = predict_labels(dev_data, svm_clf, evidence_embeddings_all)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m ev_id, ev_data \u001b[39min\u001b[39;00m evidence_data\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     36\u001b[0m     evidence_data[ev_id]\u001b[39m=\u001b[39m preprocess(ev_data)\n\u001b[1;32m---> 39\u001b[0m train_sp, temp_data \u001b[39m=\u001b[39m train_test_split(train_data, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[0;32m     40\u001b[0m val_data, test_data \u001b[39m=\u001b[39m train_test_split(temp_data, test_size\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HMY\\anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2585\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[1;32m-> 2585\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\n\u001b[0;32m   2586\u001b[0m     chain\u001b[39m.\u001b[39;49mfrom_iterable(\n\u001b[0;32m   2587\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m arrays\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HMY\\anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2587\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[0;32m   2585\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2586\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2587\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HMY\\anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\utils\\__init__.py:358\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[39mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 358\u001b[0m     \u001b[39mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[1;32mc:\\Users\\HMY\\anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\utils\\__init__.py:212\u001b[0m, in \u001b[0;36m_list_indexing\u001b[1;34m(X, key, key_dtype)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(compress(X, key))\n\u001b[0;32m    211\u001b[0m \u001b[39m# key is a integer array-like of key\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[39mreturn\u001b[39;00m [X[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m key]\n",
      "File \u001b[1;32mc:\\Users\\HMY\\anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\utils\\__init__.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(compress(X, key))\n\u001b[0;32m    211\u001b[0m \u001b[39m# key is a integer array-like of key\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[39mreturn\u001b[39;00m [X[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import InputExample, losses\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "bert_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(bert_name)\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "path = \"project-data\"\n",
    "def load_data(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "train_data = load_data(os.path.join(path, 'train-claims.json'))\n",
    "dev_data = load_data(os.path.join(path, 'dev-claims.json'))\n",
    "evidence_data = load_data(os.path.join(path, 'evidence.json'))\n",
    "# Preprocess the data\n",
    "for claim_id, claim_data in train_data.items():\n",
    "    train_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "    for i, evidence_id in enumerate(claim_data['evidences']):\n",
    "        train_data[claim_id]['evidences'][i] = preprocess(evidence_data[evidence_id])\n",
    "\n",
    "for claim_id, claim_data in dev_data.items():\n",
    "    dev_data[claim_id]['claim_text'] = preprocess(claim_data['claim_text'])\n",
    "    for i, evidence_id in enumerate(claim_data['evidences']):\n",
    "        dev_data[claim_id]['evidences'][i] = preprocess(evidence_data[evidence_id])\n",
    "\n",
    "\n",
    "for ev_id, ev_data in evidence_data.items():\n",
    "    evidence_data[ev_id]= preprocess(ev_data)\n",
    "\n",
    "data_list = [{\"id\": k, **v} for k, v in train_data.items()]\n",
    "train_sp, temp_data = train_test_split(data_list, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "import random\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, claims_data, evidence_data, tokenizer):\n",
    "        self.claims_data = claims_data\n",
    "        self.evidence_data = evidence_data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claims_data) * 2  # Generating a pair of similar and dissimilar sentences for each claim\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        claim_id = list(self.claims_data.keys())[idx // 2]\n",
    "        claim_text = self.claims_data[claim_id][\"claim_text\"]\n",
    "        evidence_ids = self.claims_data[claim_id]['evidences']\n",
    "\n",
    "        if idx % 2 == 0:\n",
    "            # For even indices, return a similar claim-evidence pair\n",
    "            evidence_text = self.evidence_data[f'evidence-{random.choice(evidence_ids)}']\n",
    "            label = 1.0\n",
    "        else:\n",
    "            # For odd indices, return a dissimilar claim-evidence pair\n",
    "            random_evidence_id = random.choice(list(self.evidence_data.keys()))\n",
    "            while f'evidence-{random_evidence_id}' in evidence_ids:\n",
    "                random_evidence_id = random.choice(list(self.evidence_data.keys()))\n",
    "            evidence_text = self.evidence_data[random_evidence_id]\n",
    "            label = 0.0\n",
    "\n",
    "        claim_encoding = self.tokenizer(claim_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "        evidence_encoding = self.tokenizer(evidence_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "        return {\n",
    "            'claim_input_ids': claim_encoding['input_ids'].squeeze(0),\n",
    "            'claim_attention_mask': claim_encoding['attention_mask'].squeeze(0),\n",
    "            'evidence_input_ids': evidence_encoding['input_ids'].squeeze(0),\n",
    "            'evidence_attention_mask': evidence_encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "bert_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = SiameseDataset(train_sp, evidence_data, tokenizer)\n",
    "val_dataset = SiameseDataset(val_data, evidence_data, tokenizer)\n",
    "# Create a dataloader for the training data\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "val_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "# Create a loss function for the siamese network\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# Define the loss function, optimizer and scheduler\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)  # Decrease learning rate by 0.1 every 2 epochs\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get the inputs and labels\n",
    "        claim_input_ids = batch['claim_input_ids'].to(device)\n",
    "        claim_attention_mask = batch['claim_attention_mask'].to(device)\n",
    "        evidence_input_ids = batch['evidence_input_ids'].to(device)\n",
    "        evidence_attention_mask = batch['evidence_attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device).unsqueeze(-1)\n",
    "\n",
    "        # Forward pass\n",
    "        claim_outputs = model(claim_input_ids, attention_mask=claim_attention_mask).last_hidden_state[:,0,:]\n",
    "        evidence_outputs = model(evidence_input_ids, attention_mask=evidence_attention_mask).last_hidden_state[:,0,:]\n",
    "        loss = criterion(claim_outputs, evidence_outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {avg_train_loss}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            claim_input_ids = batch['claim_input_ids'].to(device)\n",
    "            claim_attention_mask = batch['claim_attention_mask'].to(device)\n",
    "            evidence_input_ids = batch['evidence_input_ids'].to(device)\n",
    "            evidence_attention_mask = batch['evidence_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device).unsqueeze(-1)\n",
    "\n",
    "            claim_outputs = model(claim_input_ids, attention_mask=claim_attention_mask).last_hidden_state[:,0,:]\n",
    "            evidence_outputs = model(evidence_input_ids, attention_mask=evidence_attention_mask).last_hidden_state[:,0,:]\n",
    "            loss = criterion(claim_outputs, evidence_outputs, labels)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "    # Checkpoint the model if it has the best validation loss so far\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
